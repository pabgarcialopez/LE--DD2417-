{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbf0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /home/jovyan/word-prediction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "dir = os.getcwd().split(\"/\")[-1]\n",
    "if dir == \"ngram\":\n",
    "    os.chdir(os.path.expanduser(\"../\"))\n",
    "print(f'Project root dir: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e23eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MIN_N, MAX_N, START_SYMBOL, NUM_PREDICTIONS, TRAINED_PATH, EXTRINSIC_EVAL_SIZE\n",
    "from prepare import tokenize_sentences, train_val_test_split, prepend_start_symbol\n",
    "from evaluate import evaluate_extrinsic\n",
    "from gui import get_gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410a957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input corpus...\n",
      "Tokenization ready.\n",
      "Train dataset size: 51035\n",
      "Validation dataset size: 17012\n",
      "Test dataset size: 17012\n"
     ]
    }
   ],
   "source": [
    "trained_grams = [TRAINED_PATH + fn for fn in os.listdir(TRAINED_PATH) if \"model\" in fn]\n",
    "sentences = tokenize_sentences(\"data/HP_all.txt\")\n",
    "train_sentences, _, test_sentences = train_val_test_split(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43662213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramTester(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        This class reads a language model file and a test file, and computes\n",
    "        the entropy of the latter.\n",
    "        \"\"\"\n",
    "\n",
    "        # Size of grams\n",
    "        self.n = n\n",
    "\n",
    "        # Sentences from the test corpus.\n",
    "        self.sentences = None\n",
    "\n",
    "        # Sentence starter that adds 'padding' to each sentence\n",
    "        self.sentence_starter = [START_SYMBOL] * (n - 1)\n",
    "\n",
    "        # Collection of n-gram counts (pos i corresponds to (i+1)-ngrams)\n",
    "        self.ngram_counts = [defaultdict(int) for _ in range(n)]\n",
    "\n",
    "        # Collection of n-gram probabilities (pos i -> (i+1)-ngrams)\n",
    "        # For k = 1, ..., n:\n",
    "        # ngrams[k - 1][(w_1, ..., w_k] = log P(w_k | w_{k-1}, ..., w_1)\n",
    "        self.ngram_probs = [defaultdict(int) for _ in range(n)]\n",
    "\n",
    "        # For each word, its ID.\n",
    "        self.word2id = {}\n",
    "\n",
    "        # For each ID, its word\n",
    "        self.id2word = {}\n",
    "\n",
    "        # Vocabulary size: mumber of unique words.\n",
    "        self.num_unique_words = 0\n",
    "\n",
    "        # Total number of words from corpus\n",
    "        self.num_total_words = 0\n",
    "\n",
    "        # The entropy of the test corpus.\n",
    "        self.entropy = 0\n",
    "\n",
    "        # Linear interpolation weights\n",
    "        self.lambdas = self._get_lambdas()\n",
    "\n",
    "    def read_model(self, file):\n",
    "        \"\"\"\n",
    "        Reads the contents of the language model file into the appropriate data structures.\n",
    "\n",
    "        :param f: The name of the language model file.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            with io.open(file, mode='r', encoding='utf-8-sig') as f:\n",
    "                self.num_unique_words, self.num_total_words = map(int, f.readline().strip().split(' '))\n",
    "\n",
    "                self.word2id[START_SYMBOL] = 1\n",
    "                self.id2word[1] = START_SYMBOL\n",
    "\n",
    "                # Start with 1-grams.\n",
    "                k = 0\n",
    "\n",
    "                num_kgram_lines = int(f.readline().strip())\n",
    "                while num_kgram_lines != -1:\n",
    "                    for i in range(num_kgram_lines):\n",
    "                        if k == 0:\n",
    "                            id, token, unigram_count, log_prob = f.readline().strip().split(' ')\n",
    "                            self.word2id[token] = int(id)\n",
    "                            self.id2word[int(id)] = token\n",
    "                            self.ngram_counts[0][(token,)] = int(unigram_count)\n",
    "                            self.ngram_probs[0][(token,)] = float(log_prob)\n",
    "\n",
    "                        else:\n",
    "                            kgram_info = f.readline().strip().split(' ')\n",
    "                            log_prob = float(kgram_info[-1])\n",
    "                            kgram_count = int(kgram_info[-2])\n",
    "                            ids = list(map(int, kgram_info[:-2]))\n",
    "                            words = tuple(self.id2word[id] for id in ids)\n",
    "                            self.ngram_counts[k][words] = kgram_count\n",
    "                            self.ngram_probs[k][words] = log_prob\n",
    "\n",
    "                    # Move on to the next k-grams\n",
    "                    k += 1\n",
    "                    num_kgram_lines = int(f.readline().strip())\n",
    "\n",
    "        except IOError:\n",
    "            print(\"Couldn't find the model file {}\".format(f))\n",
    "\n",
    "    def process_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Reads and processes test sentences one word at a time.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.sentences = prepend_start_symbol(sentences, self.sentence_starter)\n",
    "            self.entropy = 0\n",
    "            for i in tqdm(range(len(self.sentences)), desc=\"Computing entropy\", colour='green'):\n",
    "                self._accumulate_entropy(tuple(self.sentences[i]))\n",
    "            return self.entropy\n",
    "\n",
    "        except IOError:\n",
    "            print('Error reading testfile')\n",
    "\n",
    "    def read_extra_unigram(self, word):\n",
    "        if word not in self.word2id:\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            self.id2word[len(self.id2word)] = word\n",
    "            self.ngram_counts[0][(word,)] += 1\n",
    "            self.ngram_probs[0][(word,)] = -float('inf')\n",
    "\n",
    "    # ==================== INTRINSIC EVALUATION ====================\n",
    "\n",
    "    def _accumulate_entropy(self, words):\n",
    "        \"\"\"\n",
    "        Computes entropy of a sentence with linear interpolation.\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(words, tuple)\n",
    "\n",
    "        # For easier readability\n",
    "        n = self.n\n",
    "        e = math.exp\n",
    "        P = self.ngram_probs\n",
    "\n",
    "        # For each possible n-gram in the sentence\n",
    "        for i in range(n - 1, len(words)):\n",
    "            prob = self.lambdas[n]\n",
    "            for r in range(n):  # r = 0, ..., n-1\n",
    "                if words[i-n+r+1:i+1] in P[n-r-1]:\n",
    "                    prob += self.lambdas[r] * e(P[n-r-1][words[i-n+r+1:i+1]])\n",
    "            self.entropy += (-1) * prob * math.log2(prob)\n",
    "        return prob\n",
    "\n",
    "    def _get_lambdas(self, first_lambda=0.9):\n",
    "        \"\"\"\n",
    "        Computes decreasing lambdas for linear interpolation.\n",
    "        \"\"\"\n",
    "\n",
    "        lambdas = [first_lambda]\n",
    "        remaining_sum = 1 - first_lambda\n",
    "        factor = 5  # Factor to decrease first lambda\n",
    "\n",
    "        for i in range(self.n):\n",
    "            next_lambda = remaining_sum / factor\n",
    "            lambdas.append(next_lambda)\n",
    "            remaining_sum -= next_lambda\n",
    "            factor *= 20  # Increase the factor to make next lambda smaller\n",
    "\n",
    "        # Divide up the remainder between all components except for the last.\n",
    "        for i in range(len(lambdas) - 1):\n",
    "            lambdas[i] += remaining_sum / (len(lambdas) - 1)\n",
    "        return lambdas\n",
    "\n",
    "    # ==================== EXTRINSIC EVALUATION ====================\n",
    "\n",
    "    def _select_candidates(self, candidates, weights, k):\n",
    "\n",
    "        # Create classes\n",
    "        classes = defaultdict(list)\n",
    "        for i in range(len(candidates)):\n",
    "            classes[weights[i]].append(candidates[i])\n",
    "\n",
    "        i = 0\n",
    "        chosen = []\n",
    "        while k > 0 and len(classes) > 0:\n",
    "\n",
    "            # Find class with highest weight\n",
    "            max_weight = max(classes.keys())\n",
    "            if max_weight == 0:\n",
    "                break\n",
    "\n",
    "            # Choose from this class\n",
    "            weight_class = classes[max_weight]\n",
    "            num_chosen = min(k, len(weight_class))\n",
    "            chosen += random.sample(weight_class, num_chosen)\n",
    "\n",
    "            # Remove this class for next iteration\n",
    "            classes.pop(max_weight)\n",
    "            k -= num_chosen\n",
    "\n",
    "        return chosen\n",
    "\n",
    "    def predict(self, prev_words, partial_word, k, use_interpolation=True):\n",
    "        \"\"\"\n",
    "        Returns `k` predicted words based on the previous words and the current\n",
    "        typed partial word, which could be empty\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure the prediction fits the model\n",
    "        assert len(prev_words) < self.n\n",
    "\n",
    "        prev_words = [w.lower() for w in prev_words]\n",
    "        partial_word = partial_word.lower()\n",
    "        kgram_bucket = len(prev_words)\n",
    "\n",
    "        # Obtain candidates to follow the prev_words.\n",
    "        candidates = [key[0] for key in self.ngram_counts[0].keys()\n",
    "                      if key[0].startswith(partial_word)]\n",
    "\n",
    "        # Obtain probabilities P(candidate_i | prev_words)\n",
    "        weights = []\n",
    "        for candidate in candidates:\n",
    "            sequence = tuple(prev_words) + (candidate,)\n",
    "            if use_interpolation:\n",
    "                weights.append(self._accumulate_entropy(sequence))\n",
    "            else:\n",
    "                if sequence not in self.ngram_probs[kgram_bucket]:\n",
    "                    weights.append(0)\n",
    "                else:\n",
    "                    weights.append(math.exp(self.ngram_probs[kgram_bucket][sequence]))\n",
    "\n",
    "        chosen = self._select_candidates(candidates, weights, k)\n",
    "        return chosen, len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae69943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing 3-grams:\n",
      "Using model: model_3gram_hp_all.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing entropy: 100%|\u001b[32m██████████\u001b[0m| 17012/17012 [00:00<00:00, 34458.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 42233.58\n",
      "k=3 suggestion(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keystrokes evaluation: 100%|██████████| 1000/1000 [10:33<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Keystrokes: 19417\n",
      " All characters: 60155\n",
      " Keystroke savings: 67.72%\n",
      " Average number of possible words when correctly guessed: 8180\n",
      "k=4 suggestion(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keystrokes evaluation: 100%|██████████| 1000/1000 [10:31<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Keystrokes: 17501\n",
      " All characters: 60155\n",
      " Keystroke savings: 70.91%\n",
      " Average number of possible words when correctly guessed: 9010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(MIN_N, MAX_N + 1):\n",
    "    print(\"\\nTesting \" + str(n) + \"-grams:\")\n",
    "\n",
    "    tester = NGramTester(n)\n",
    "\n",
    "    # Get corresponding model and make tester read it.\n",
    "    gram_model = [fn for fn in trained_grams if \"model_\" + str(n) in fn][0]\n",
    "    print(\"Using model:\", gram_model.split('/')[-1])\n",
    "    tester.read_model(gram_model)\n",
    "\n",
    "    # Provide testing files to tester\n",
    "    entropy = tester.process_sentences(test_sentences)\n",
    "    print(\"Entropy:\", format(entropy, '.2f'))\n",
    "    evaluate_extrinsic(test_sentences[:EXTRINSIC_EVAL_SIZE], tester, NUM_PREDICTIONS, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9650f-b899-4448-8306-ec5ffe1d2162",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48fd4b57-b755-47b0-b215-062d5e24d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://f8c20f2a804921c132.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f8c20f2a804921c132.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = MAX_N\n",
    "model = NGramTester(N)\n",
    "trained_grams = [TRAINED_PATH + fn for fn in os.listdir(TRAINED_PATH) if \"model\" in fn]\n",
    "gram_model = [fn for fn in trained_grams if \"model_\" + str(N) in fn][0]\n",
    "model.read_model(gram_model)\n",
    "get_gui(model, f'{model.n}-gram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8131d4-98ca-42dc-b937-4106bdf70f98",
   "metadata": {},
   "source": [
    "## Extra evaluation on completely different text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179c48c7-273b-4f81-aa98-166255dd071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input corpus...\n",
      "Tokenization ready.\n",
      "Train dataset size: 0\n",
      "Validation dataset size: 0\n",
      "Test dataset size: 33672\n",
      "k=4 suggestion(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keystrokes evaluation: 100%|██████████| 1000/1000 [19:05<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Keystrokes: 76361\n",
      " All characters: 119123\n",
      " Keystroke savings: 35.90%\n",
      " Average number of possible words when correctly guessed: 3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "def load_glove_words(embedding_file, model):\n",
    "    with codecs.open(embedding_file, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0].lower()\n",
    "            model.read_extra_unigram(word)\n",
    "\n",
    "N = 5\n",
    "model = NGramTester(N)\n",
    "load_glove_words('data/glove.6B.50d.txt', model)\n",
    "guardian_test = tokenize_sentences(\"data/guardian_test.txt\", [START_SYMBOL]*N)\n",
    "_, _, guardian_test = train_val_test_split(guardian_test, 0, 0, 1)\n",
    "evaluate_extrinsic(guardian_test[:EXTRINSIC_EVAL_SIZE], tester, NUM_PREDICTIONS[-1:], n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
