{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea9c0d4-e0da-4037-b67c-2970cac42ed5",
   "metadata": {},
   "source": [
    "# Word Predictor: n-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284d1ca1-f2c4-4852-9362-a35d2af7b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fad3a6-13b8-44aa-8128-e16b43f4c361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:39:30.722470Z",
     "start_time": "2024-05-17T07:39:30.708047Z"
    }
   },
   "outputs": [],
   "source": [
    "from config import MIN_N, MAX_N, TRAINED_PATH, START_SYMBOL\n",
    "from prepare import tokenize_sentences, train_val_test_split, prepend_start_symbol, buildWordIdMappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6144d10-2559-41ec-9fcc-5bce4fff002b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:38:03.748200Z",
     "start_time": "2024-05-17T07:38:03.683595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input corpus...\n",
      "Tokenization ready.\n",
      "Train dataset size: 51035\n",
      "Validation dataset size: 17012\n",
      "Test dataset size: 17012\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenize_sentences(\"data/HP_all.txt\")\n",
    "train_sentences, _, test_sentences = train_val_test_split(sentences)\n",
    "word2id, id2word = buildWordIdMappings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7b3443-0d9d-4f33-ac51-92f40637602b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:37:35.989393Z",
     "start_time": "2024-05-17T07:37:35.963259Z"
    }
   },
   "outputs": [],
   "source": [
    "class NGramTrainer(object):\n",
    "\n",
    "    def __init__(self, n, word2id, id2word):\n",
    "        \"\"\"\n",
    "        NGramTrainer constructor\n",
    "\n",
    "        :param n: Size of grams\n",
    "        \"\"\"\n",
    "        # Size of grams\n",
    "        self.n = n\n",
    "\n",
    "        # For each word, its ID.\n",
    "        self.word2id = word2id\n",
    "\n",
    "        # For each ID, its word\n",
    "        self.id2word = id2word\n",
    "\n",
    "        # Sentence starter that adds 'padding' to each sentence\n",
    "        self.sentence_starter = [START_SYMBOL] * (n - 1)\n",
    "\n",
    "        # Sentence tokens from corpus\n",
    "        self.sentences = None\n",
    "\n",
    "        # Collection of n-gram counts (pos i corresponds to (i+1)-ngrams)\n",
    "        self.ngrams = [defaultdict(int) for _ in range(n)]\n",
    "\n",
    "        # Total number of words from corpus\n",
    "        self.num_total_words = 0\n",
    "\n",
    "    def process_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Processes the given sentences.\n",
    "        \"\"\"\n",
    "        self.sentences = prepend_start_symbol(sentences, self.sentence_starter)\n",
    "        for i in tqdm(range(len(self.sentences)), desc=\"Processing corpus\", colour='green'):\n",
    "            self._process_tokens(tuple(self.sentences[i]))\n",
    "        return self.sentences\n",
    "\n",
    "    def _process_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Processes the list of tokens, and\n",
    "        adjusts the ngram counts.\n",
    "\n",
    "        :param tokens: The list of tokens to be processed.\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(tokens, tuple)\n",
    "\n",
    "        self.num_total_words += len(tokens)\n",
    "\n",
    "        # We will count one start symbol per sentence\n",
    "        self.ngrams[self.n - 2][tuple(self.sentence_starter)] += 1\n",
    "\n",
    "        # Iterate over all possible n-grams\n",
    "        for i in range(self.n - 1, len(tokens)):\n",
    "            # Obtain the n-gram stretching from pos i-n+1 to i --> interval [i-n+1, i+1)\n",
    "            ngram = tokens[i - self.n + 1:i + 1]\n",
    "\n",
    "            # Update the count for each l-gram, l = 1, ..., n\n",
    "            for k in range(self.n):  # k = 0, ..., n-1\n",
    "                self.ngrams[k][ngram[self.n - 1 - k:i + 1]] += 1\n",
    "\n",
    "    def _get_stats(self):\n",
    "        \"\"\"\n",
    "        Returns the model statistics\n",
    "        \"\"\"\n",
    "\n",
    "        log = math.log\n",
    "\n",
    "        # Initial row\n",
    "        rows = [str(len(self.word2id)) + \" \" + str(self.num_total_words)]\n",
    "\n",
    "        # For each k-grams, print their stats\n",
    "        for k in range(self.n):\n",
    "\n",
    "            # Get the k-grams\n",
    "            kgrams = self.ngrams[k]\n",
    "\n",
    "            # Record how many lines are gonna follow\n",
    "            rows.append(str(len(kgrams)))\n",
    "\n",
    "            # For each kgram (tuple) in the kgrams dict\n",
    "            for kgram in kgrams:\n",
    "\n",
    "                # Transform the words into string ids\n",
    "                ids = ' '.join(str(self.word2id[word]) for word in kgram)\n",
    "\n",
    "                # Compute the (log) probability\n",
    "                # P(w_i | w_{i-n+1}, ..., w_{i-1}) =\n",
    "                # c(w_{i-n+1}, ..., w_{i-1}, w_i) / c(w_{i-n+1}, ..., w_{i-1})\n",
    "\n",
    "                # Get the number of occurrences of this kgram\n",
    "                kgram_count = kgrams[kgram]\n",
    "                if k == 0:  # Uni-gram --> Use log_prob for unigram_count\n",
    "                    ids += ' ' + kgram[0]  # Append word.\n",
    "                    log_prob = log(kgram_count) - log(self.num_total_words)\n",
    "                else:  # Dealing with 2, 3, ... -grams.\n",
    "                    # If the previous kgram doesn't exist (start symbols)\n",
    "                    if kgram[:-1] not in self.ngrams[k - 1]:\n",
    "                        log_prob = -float('inf')  # So that e^(-inf) = 0\n",
    "                    else:\n",
    "                        prev_kgram_count = self.ngrams[k - 1][kgram[:-1]]\n",
    "                        log_prob = log(kgram_count) - log(prev_kgram_count)\n",
    "                log_prob = format(log_prob, '.15f')\n",
    "                rows.append(ids + \" \" + str(kgram_count) + \" \" + str(log_prob))\n",
    "        rows.append(str(-1))  # EOF\n",
    "        return rows\n",
    "\n",
    "    def save_model(self, file):\n",
    "        \"\"\"\n",
    "        Save model stats in the provided file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Saving model...\")\n",
    "            with io.open(file, mode='w', encoding='utf-8-sig') as f:\n",
    "                for row in self._get_stats():\n",
    "                    f.write(row + '\\n')\n",
    "            print(\"Model saved!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"The file\", file, \" was not found.\")\n",
    "        except IOError:\n",
    "            print(\"An IOError occurred while saving the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d245627-a03c-4e1f-bde6-62d5c38e3fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T07:38:03.748200Z",
     "start_time": "2024-05-17T07:38:03.683595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 2-gram model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpus: 100%|\u001b[32m██████████\u001b[0m| 51035/51035 [00:00<00:00, 56234.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved!\n",
      "\n",
      "Training 3-gram model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpus: 100%|\u001b[32m██████████\u001b[0m| 51035/51035 [00:01<00:00, 38752.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved!\n",
      "\n",
      "Training 4-gram model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpus: 100%|\u001b[32m██████████\u001b[0m| 51035/51035 [00:01<00:00, 26662.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved!\n",
      "\n",
      "Training 5-gram model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpus: 100%|\u001b[32m██████████\u001b[0m| 51035/51035 [00:02<00:00, 21032.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "for n in range(MIN_N, MAX_N + 1):\n",
    "    print(\"\\nTraining \" + str(n) + \"-gram model:\")\n",
    "    trainer = NGramTrainer(n, word2id, id2word)\n",
    "    trainer.process_sentences(train_sentences)\n",
    "    trainer.save_model(TRAINED_PATH + \"model_\" + str(n) + \"gram_hp_all.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
